{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch \\#“Θα κάνω κάτι με PINNs” import torch.nn as nn #Νευρωνικά\n",
    "δίκτυα import matplotlib.pyplot as plt\n",
    "\n",
    "a=1 #ΝΔ net = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10,\n",
    "1)) #Το Linear σημαίνει ξεκινάω με μία ευθεία ως χαζή 1η προσέγγιση, 10\n",
    "νευρώνες #Το Tanh() είναι η σιγμοειδής, εναλλακτικά Sigmoid(), ReLU()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.01) #Άλλαξε κάθε βάρος\n",
    "(default, δεν ξέρω αρκετά για να το αμφισβητήσω), lr: βήμα μάθησης for\n",
    "epoch in range(1000): #Πλήθος επαναλήψεων x=torch.linspace(0, 2,\n",
    "100).reshape(-1, 1).requires_grad\\_(True) y=net(x) #Πρόβλεψη\n",
    "dy_dx=torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y),\n",
    "create_graph=True)\\[0\\]#Παράγωγος loss_ode=torch.mean((dy_dx-a\\*y)**2)\n",
    "#Συνάρτηση σφάλματος της ΣΔΕ y0=net(torch.tensor(\\[\\[0.0\\]\\])) #Φτιάξε\n",
    "μία “θέση” για να μπει η αρχική συνθήκη loss_bc=(y0-1)**2 #Συνάρτηση\n",
    "σφάλματος των ΣΣ (εδώ ΑΣ y0=1)- “soft constrain” loss=loss_ode+loss_bc\n",
    "#Εδώ, w1=w2=1 optimizer.zero_grad() #Μηδενισμός προηγούμενων κλίσεων\n",
    "loss.backward() #Υπολογισμός κλίσεων της loss ως προς όλες τις\n",
    "παραμέτρους (net) & αποθήκευση στο .grad (SOS) optimizer.step()\n",
    "#Ενημέρωση του βήματος, τυπικό net.eval() #Plot x_test=torch.linspace(0,\n",
    "2, 100).reshape(-1, 1) x_np=x_test.numpy() #Το .numpy() το κάνει νορμάλ\n",
    "πίνακα y_pred=net(x_test).detach().numpy() #detach()= ξέχνα το ιστορικό\n",
    "plt.plot(x_np,y_pred)"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
